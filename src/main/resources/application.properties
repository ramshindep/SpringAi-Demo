spring.application.name=springAiDemo
# Spring AI configuration for Ollama
spring.ai.ollama.enabled=true

# The base URL of your local Ollama server (default is localhost:11434)
spring.ai.ollama.base-url=http://localhost:11434

# The name of the Ollama model you want to use
spring.ai.ollama.model=ollama3

# Optional: Timeout settings (in milliseconds)
spring.ai.ollama.connect-timeout=10000
spring.ai.ollama.read-timeout=60000
